{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tuning Vision Transformers with LoRA for Binary Classification\n",
    "\n",
    "In this notebook, we implement a robust pipeline for fine-tuning a pre-trained **Vision Transformer (ViT)** on a custom dataset using **Low-Rank Adaptation (LoRA)**.\n",
    "\n",
    "### Technical Objectives\n",
    "1.  **Architecture**: Leverage `vit-base-patch16-224-in21k` as a feature extractor backbone.\n",
    "2.  **Parameter Efficiency**: Implement LoRA to reduce traininable parameters by ~99% while maintaining performance.\n",
    "3.  **Deployment**: Construct an inference pipeline capable of real-time prediction using Gradio.\n",
    "\n",
    "### Stack\n",
    "*   **Hugging Face Transformers**: Model architecture and pre-trained weights.\n",
    "*   **PEFT**: Parameter-Efficient Fine-Tuning implementation.\n",
    "*   **PyTorch**: Deep learning backend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "Installing necessary dependencies for the transformer ecosystem and dynamic dataset handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torch transformers peft datasets pillow numpy gradio evaluate scikit-learn kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute backend: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import kagglehub\n",
    "from datasets import Dataset\n",
    "\n",
    "# Auto-select CUDA if available for accelerated training\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Compute backend: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ELT Pipeline (Extract, Load, Transform)\n",
    "\n",
    "We ingest the **[`louisiana-flood-2016`](https://www.kaggle.com/datasets/rahultp97/louisiana-flood-2016)** dataset sourced from Kaggle. \n",
    "\n",
    "The pipeline involves:\n",
    "1.  **Extraction**: Programmatic download via `kagglehub`.\n",
    "2.  **Label Parsing**: Binary labels are encoded from filenames (`_1.png` -> Positive Class).\n",
    "3.  **IO Handling**: We store file paths to manage memory consumption efficiently, loading images into RAM only during the transformation stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing data download...\n",
      "Dataset cache path: /root/.cache/kagglehub/datasets/rahultp97/louisiana-flood-2016/versions/4\n",
      "Train samples: 270 | Test samples: 52\n"
     ]
    }
   ],
   "source": [
    "print(\"Initializing data download...\")\n",
    "DATA_DIR = kagglehub.dataset_download(\"rahultp97/louisiana-flood-2016\")\n",
    "print(f\"Dataset cache path: {DATA_DIR}\")\n",
    "\n",
    "def extract_metadata(data_dir):\n",
    "    \"\"\"Parses directory structure to return image paths and corresponding binary labels.\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(('.png', '.jpg')):\n",
    "                image_paths.append(os.path.join(root, file))\n",
    "                # Label Encoding: 1 = Flooded, 0 = Non-Flooded\n",
    "                labels.append(1 if file.endswith(\"_1.png\") else 0)\n",
    "    return image_paths, labels\n",
    "\n",
    "# Split extraction assumes standard train/test directory structure\n",
    "train_paths, train_labels = extract_metadata(os.path.join(DATA_DIR, \"train\"))\n",
    "test_paths, test_labels = extract_metadata(os.path.join(DATA_DIR, \"test\"))\n",
    "\n",
    "print(f\"Train samples: {len(train_paths)} | Test samples: {len(test_paths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture Initialization\n",
    "\n",
    "We utilize a **ViT-Base** architecture. \n",
    "*   **Patch Size**: 16x16 (Standard for 224x224 input).\n",
    "*   **Classification Head**: The original 21k-class MLP head is replaced with a randomly initialized binary classification head (`num_labels=2`).\n",
    "*   **Processor**: Handles normalization (ImageNet mean/std) and resizing to ensure tensor compatibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification, ViTImageProcessor\n",
    "\n",
    "MODEL_CKPT = \"google/vit-base-patch16-224-in21k\"\n",
    "\n",
    "# Initialize Preprocessor for ImageNet standardization\n",
    "processor = ViTImageProcessor.from_pretrained(MODEL_CKPT)\n",
    "\n",
    "# Initialize Model with custom Classification Head\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    MODEL_CKPT,\n",
    "    num_labels=2,\n",
    "    id2label={0: \"Non-Flooded\", 1: \"Flooded\"},\n",
    "    label2id={\"Non-Flooded\": 0, \"Flooded\": 1}\n",
    ")\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Parameter-Efficient Fine-Tuning (PEFT) Configuration\n",
    "\n",
    "We implement **LoRA (Low-Rank Adaptation)**. Instead of full-parameter fine-tuning, we inject low-rank decomposition matrices into the attention blocks.\n",
    "\n",
    "### LoRA Hyperparameters\n",
    "*   **Rank (`r=16`)**: Determines the dimensionality of the low-rank matrices. Higher `r` allows more expressivity but increases parameters.\n",
    "*   **Target Modules**: We adapt the `query` and `value` projections of the Self-Attention mechanism, which has empirically shown high yield for transfer learning tasks.\n",
    "*   **Alpha**: Scaling factor for the learned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 591,362 || all params: 86,391,556 || trainable%: 0.6845\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"classifier\"] # We must train the new head fully\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**: The output confirms we are training <1% of the total parameters. This allows for significantly higher batch sizes and lower memory VRAM footprint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop Execution\n",
    "We utilize the Hugging Face `Trainer` API for abstraction of the training loop, gradient accumulation, and evaluation.\n",
    "\n",
    "### Key Components:\n",
    "*   **Collate Function**: Dynamically stacks tensors into batches.\n",
    "*   **Transform**: Applies the ViT processor to the PIL images on-the-fly.\n",
    "*   **Metric**: Accuracy is used as the primary evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='102' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/102 00:11, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.407400</td>\n",
       "      <td>0.155756</td>\n",
       "      <td>0.942308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.002600</td>\n",
       "      <td>0.011570</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.072500</td>\n",
       "      <td>0.014748</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=102, training_loss=0.15123242276204824, metrics={'train_runtime': 35.7454, 'train_samples_per_second': 22.66, 'train_steps_per_second': 2.854, 'total_flos': 6.320113196802048e+16, 'train_loss': 0.15123242276204824, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n",
    "# Data Transformation: PIL -> Tensor\n",
    "def transform_batch(batch):\n",
    "    # return_tensors='pt' ensures PyTorch compatibility\n",
    "    inputs = processor([x for x in batch['image']], return_tensors='pt')\n",
    "    inputs['label'] = batch['label']\n",
    "    return inputs\n",
    "\n",
    "# Custom Collate: Handles batch stacking\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['label'] for x in batch])\n",
    "    }\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = evaluate.load(\"accuracy\")\n",
    "    preds = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=preds, references=eval_pred.label_ids)\n",
    "\n",
    "# Dataset Generation Wrapper\n",
    "def create_hf_dataset(paths, labels):\n",
    "    def gen():\n",
    "        for p, l in zip(paths, labels):\n",
    "            yield {\"image\": Image.open(p).convert(\"RGB\"), \"label\": l}\n",
    "    return Dataset.from_generator(gen)\n",
    "\n",
    "# Instantiating Pipelines\n",
    "train_ds = create_hf_dataset(train_paths, train_labels)\n",
    "test_ds = create_hf_dataset(test_paths, test_labels)\n",
    "\n",
    "# Apply transforms\n",
    "train_ds = train_ds.with_transform(transform_batch)\n",
    "test_ds = test_ds.with_transform(transform_batch)\n",
    "\n",
    "# Training Hyperparameters\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",                 # Directory to store checkpoints and logs\n",
    "    per_device_train_batch_size=8,           # Small batch size to fit GPU memory\n",
    "    eval_strategy=\"epoch\",                   # Run evaluation after each epoch\n",
    "    save_strategy=\"epoch\",                   # Save model checkpoints per epoch\n",
    "    num_train_epochs=3,                      # Limited epochs to prevent overfitting\n",
    "    learning_rate=5e-3,                      # Higher LR suitable for LoRA fine-tuning\n",
    "    remove_unused_columns=False,             # Required for custom data collator\n",
    "    logging_steps=10                         # Log training metrics every N steps\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                             # LoRA-adapted ViT model\n",
    "    args=args,                               # Training configuration\n",
    "    train_dataset=train_ds,                  # Training split\n",
    "    eval_dataset=test_ds,                    # Validation split\n",
    "    data_collator=collate_fn,                # Custom batch collation logic\n",
    "    compute_metrics=compute_metrics           # Evaluation metrics (e.g., accuracy, F1)\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inference Deployment\n",
    "We encapsulate the forward pass logic into a prediction function and expose it via a REST-like interface using Gradio. This serves as a rapid prototype for model validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://cd5febd8f4a2980a36.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://cd5febd8f4a2980a36.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def inference_pipeline(image):\n",
    "    if image is None: return \"No input.\"\n",
    "    \n",
    "    # Preprocessing\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Inference (No Gradient Calculation needed)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Softmax for probability distribution\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    \n",
    "    # Post-processing\n",
    "    score = probs.max().item()\n",
    "    label_idx = probs.argmax().item()\n",
    "    label = model.config.id2label[label_idx]\n",
    "    \n",
    "    return f\"{label} (Confidence: {score:.4f})\"\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=inference_pipeline,\n",
    "    inputs=gr.Image(type=\"pil\", label=\"Input Image\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"ViT Flood Detection Module\",\n",
    "    description=\"Real-time inference using LoRA-tuned Vision Transformer.\"\n",
    ")\n",
    "\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing server running on port: 7860\n"
     ]
    }
   ],
   "source": [
    "gr.close_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thank you for your time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
